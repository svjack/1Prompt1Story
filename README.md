<h1 align="center">
  <!-- <br>
  <a href="http://www.amitmerchant.com/electron-markdownify"><img src="https://raw.githubusercontent.com/amitmerchant1990/electron-markdownify/master/app/img/markdownify.png" alt="Markdownify" width="200"></a>
  <br> -->
  ğŸ”¥(ICLR 2025) One-Prompt-One-Story: Free-Lunch Consistent Text-to-Image Generation Using a Single Prompt
  <br>
</h1>


<div align="center">

<a href="https://c5e9af216625826dc6.gradio.live" style="display: inline-block;">
    <img src="./resource/gradio.svg" alt="demo" style="height: 20px; vertical-align: middle;">
</a>&nbsp;
<a href="https://arxiv.org/abs/2501.13554" style="display: inline-block;">
    <img src="https://img.shields.io/badge/arXiv%20paper-2406.06525-b31b1b.svg" alt="arXiv" style="height: 20px; vertical-align: middle;">
</a>&nbsp;
<a href="https://byliutao.github.io/1Prompt1Story.github.io/" style="display: inline-block;">
    <img src="https://img.shields.io/badge/Project_page-More_visualizations-green" alt="project page" style="height: 20px; vertical-align: middle;">
</a>&nbsp;

</div>


<p align="center">
  <a href="#key-features">Key Features</a> â€¢
  <a href="#how-to-use">How To Use</a> â€¢
  <a href="#license">License</a> â€¢
  <a href="#Citation">Citation</a> 
</p>

<p align="center">
  <img src="./resource/photo.gif" alt="screenshot" />
</p>


## Key Features

* Consistent Identity Image Generation.
* Gradio Demo.
* Consistory+ Benchmark: contains 200 prompt sets, with each set containing between 5 and 10 prompts, categorized into 8 superclasses: humans, animals, fantasy, inanimate, fairy tales, nature, technology.
* Benchmark Generation Code.

```bash
sudo apt-get update && sudo apt-get install git-lfs ffmpeg cbm

# Clone this repository
git clone https://github.com/svjack/1Prompt1Story

# Go into the repository
cd 1Prompt1Story

### Install dependencies ###
conda create --name 1p1s python=3.10
conda activate 1p1s

# Install ipykernel and add the environment to Jupyter
pip install ipykernel
python -m ipykernel install --user --name 1p1s --display-name "1p1s"
### Install dependencies ENDs ###

# Install PyTorch with CUDA 12.1
#pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install torch torchvision torchaudio

# Install other dependencies using pip
pip install transformers diffusers opencv-python scipy gradio==4.44.1 sympy==1.13.1
pip install "httpx[socks]" datasets

huggingface-cli login

# Run gradio demo
python app.py

# Run sample code
python main.py

# Run Consistory+ benchmark
python -m resource.gen_benchmark --save_dir ./result/benchmark --benchmark_path ./resource/consistory+.yaml
```

- One Piece Background
```python
from gradio_client import Client

client = Client("http://127.0.0.1:7860")
result = client.predict(
		model_path="cagliostrolab/animagine-xl-3.1",
		id_prompt="A quaint illustration of the environment and background in One Piece",
		frame_prompt_list="in a flower garden, building with rose, birds in a city park",
		precision="fp16",
		seed=32,
		window_length=10,
		alpha_weaken=0.01,
		beta_weaken=0.05,
		alpha_enhance=-0.01,
		beta_enhance=1,
		ipca_drop_out=0,
		use_freeu="false",
		use_same_init_noise="true",
		api_name="/main_gradio"
)
print(result)

from PIL import Image
from IPython import display

Image.open(result).save("one_piece_flower.png")
display.Image("one_piece_flower.png")
```

![one_piece_flower](https://github.com/user-attachments/assets/a7d9e136-d388-48af-a829-08ae0ecd01bd)

- Anime Background Style Compare
```python
#!huggingface-cli login
from gradio_client import Client
from PIL import Image
from datasets import Dataset
import os
from tqdm import tqdm  # å¯¼å…¥ tqdm

# é…ç½®å˜é‡
generated_images_dir = "custom_images_dir"  # å›¾ç‰‡ä¿å­˜ç›®å½•
generated_dataset_dir = "custom_dataset_dir"  # æ•°æ®é›†ä¿å­˜ç›®å½•
#series_names = ["One Piece", "Naruto", "Attack on Titan"]  # æ¼«ç”»åç§°åˆ—è¡¨
#styles = ["Anime Style", "Realistic Style", "Watercolor Style"]  # é£æ ¼é€‰é¡¹
series_names = ["One Piece",]  # æ¼«ç”»åç§°åˆ—è¡¨
styles = ["Anime Style",]  # é£æ ¼é€‰é¡¹

# åˆå§‹åŒ– Gradio å®¢æˆ·ç«¯
client = Client("http://127.0.0.1:7860")

# å®šä¹‰ examples å’Œ model_path åˆ—è¡¨
examples = {
    "Nature Scene": "Sunlight spills over the emerald grass, a gentle breeze sways blooming flowers, distant mountains fade in and out of view, and a stream whispers softly as it flows.",
    "Sci-Fi Scene": "The spaceship glides through the stars, its blue exhaust cutting through the darkness, robotic arms work tirelessly outside, and an AI voice echoes, 'Destination planet approaching.'",
    "City Nightscape": "Neon lights illuminate the streets, glass skyscrapers reflect the shimmering glow, crowds bustle through the sidewalks, and cars weave endlessly through the noise.",
    "Fantasy Adventure": "In an ancient forest where trees touch the sky, a lone traveler treads softly on moss-covered ground, magical creatures peek from behind the leaves, and a glowing portal hums with untold secrets.",
    "Historical Setting": "The cobblestone streets of the old town echo with history, horse-drawn carriages clatter past stone buildings, and market vendors call out to passersby, their stalls filled with spices, fabrics, and trinkets.",
    "Post-Apocalyptic World": "The ruins of a once-great city stretch to the horizon, broken skyscrapers loom like skeletal giants, dust swirls in the air carried by a dry wind, and a lone figure scavenges for supplies in the silence.",
    "Beach Getaway": "Waves crash against the golden shore, seagulls cry as they glide through the salty air, palm trees sway in the tropical breeze, and the sun sets, painting the sky in hues of orange and pink.",
    "Mystery Scene": "A dimly lit room holds secrets untold, bookshelves line the walls filled with dusty tomes, a single candle flickers on the desk, and long shadows dance with every movement.",
    "Space Exploration": "The astronaut floats in the vast emptiness of space, Earth a distant blue marble in the background, stars twinkle like diamonds in the void, and the silence is both peaceful and overwhelming.",
    "Medieval Battle": "The battlefield roars with the clash of steel, knights in armor charge under fluttering banners, arrows rain down from the sky, and the ground trembles under the weight of marching armies."
}

model_paths = [
    "cagliostrolab/animagine-xl-3.1", "svjack/GenshinImpact_XL_Base",
    "stabilityai/stable-diffusion-xl-base-1.0", "RunDiffusion/Juggernaut-X-v10", 
    "playgroundai/playground-v2.5-1024px-aesthetic", "SG161222/RealVisXL_V4.0", 
    "RunDiffusion/Juggernaut-XI-v11", 
    #"SG161222/RealVisXL_V5.0"
]

# åˆ›å»ºä¿å­˜å›¾ç‰‡çš„ç›®å½•
os.makedirs(generated_images_dir, exist_ok=True)

# åˆå§‹åŒ–æ•°æ®é›†åˆ—è¡¨
data = []

# ä½¿ç”¨ tqdm æ˜¾ç¤ºæ€»è¿›åº¦æ¡
total_iterations = len(examples) * len(styles) * len(series_names) * len(model_paths)
with tqdm(total=total_iterations, desc="Generating Images", unit="image") as pbar:
    # éå†æ¯ä¸ªåœºæ™¯ã€é£æ ¼ã€æ¼«ç”»åç§°å’Œæ¨¡å‹
    for scene_category, scene_prompt in examples.items():
        for style in styles:
            for series_name in series_names:
                for model_path in model_paths:
                    # åŠ¨æ€ç”Ÿæˆ id_promptï¼Œæ›¿æ¢æ¨¡æ¿å˜é‡
                    id_prompt = f"A quaint illustration of the environment and background in {series_name}, {style}"
                    
                    # è°ƒç”¨ Gradio å®¢æˆ·ç«¯ç”Ÿæˆå›¾ç‰‡
                    result = client.predict(
                        model_path=model_path,
                        id_prompt=id_prompt,
                        frame_prompt_list=scene_prompt,
                        precision="fp16",
                        seed=32,
                        window_length=10,
                        alpha_weaken=0.01,
                        beta_weaken=0.05,
                        alpha_enhance=-0.01,
                        beta_enhance=1,
                        ipca_drop_out=0,
                        use_freeu="false",
                        use_same_init_noise="true",
                        api_name="/main_gradio"
                    )
                    
                    # ä¿å­˜å›¾ç‰‡åˆ°æœ¬åœ°
                    image_name = f"{model_path.split('/')[-1]}_{scene_category.replace(' ', '_')}_{style.replace(' ', '_')}_{series_name.replace(' ', '_')}.png"
                    image_path = os.path.join(generated_images_dir, image_name)
                    Image.open(result).save(image_path)
                    
                    # å°†æ•°æ®æ·»åŠ åˆ°æ•°æ®é›†åˆ—è¡¨
                    data.append({
                        "model_name": model_path.split('/')[-1],
                        "scene_category": scene_category,
                        "scene_prompt": scene_prompt,
                        "style": style,  # é£æ ¼åˆ—
                        "series_name": series_name,  # æ¼«ç”»åç§°åˆ—
                        "image": image_path
                    })
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    pbar.update(1)

# åˆ›å»ºæ•°æ®é›†å¯¹è±¡
dataset = Dataset.from_dict({
    "model_name": [item["model_name"] for item in data],
    "scene_category": [item["scene_category"] for item in data],
    "scene_prompt": [item["scene_prompt"] for item in data],
    "style": [item["style"] for item in data],  # é£æ ¼åˆ—
    "series_name": [item["series_name"] for item in data],  # æ¼«ç”»åç§°åˆ—
    "image": [item["image"] for item in data]
})

# åˆ›å»ºä¿å­˜æ•°æ®é›†çš„ç›®å½•
os.makedirs(generated_dataset_dir, exist_ok=True)

# ä¿å­˜æ•°æ®é›†åˆ°æœ¬åœ°
dataset.save_to_disk(generated_dataset_dir)

from PIL import Image
ds = dataset.map(lambda x: {"Image": Image.open(x["image"])}).remove_columns(["image"])

ds.push_to_hub("svjack/OnePromptOneStory-AnimeStyle")

print(f"æ•°æ®é›†ç”Ÿæˆå¹¶ä¿å­˜å®Œæˆï¼å›¾ç‰‡ä¿å­˜åˆ°ï¼š{generated_images_dir}ï¼Œæ•°æ®é›†ä¿å­˜åˆ°ï¼š{generated_dataset_dir}")
```

- Build in Examples
```python
#!huggingface-cli login
from gradio_client import Client
from PIL import Image
from datasets import Dataset
import os
from tqdm import tqdm  # å¯¼å…¥ tqdm ç”¨äºè¿›åº¦æ¡

# é…ç½®å˜é‡
generated_images_dir = "custom_images_dir"  # å›¾ç‰‡ä¿å­˜ç›®å½•
generated_dataset_dir = "custom_dataset_dir"  # æ•°æ®é›†ä¿å­˜ç›®å½•

# åˆå§‹åŒ– Gradio å®¢æˆ·ç«¯
client = Client("http://127.0.0.1:7860")

# å®šä¹‰ model_path åˆ—è¡¨
model_paths = [
    "cagliostrolab/animagine-xl-3.1", 
    "svjack/GenshinImpact_XL_Base",
    "stabilityai/stable-diffusion-xl-base-1.0", 
    "RunDiffusion/Juggernaut-X-v10", 
    "playgroundai/playground-v2.5-1024px-aesthetic", 
    "SG161222/RealVisXL_V4.0", 
    "RunDiffusion/Juggernaut-XI-v11", 
    #"SG161222/RealVisXL_V5.0"
]

# åˆ›å»ºä¿å­˜å›¾ç‰‡çš„ç›®å½•
os.makedirs(generated_images_dir, exist_ok=True)

# ç›´æ¥å®šä¹‰ combinations
combinations = [
    {
        "id_prompt": "A hyper-realistic digital painting of a 16 years old girl.",
        "frame_prompt_list": [
            "in a flower garden",
            "building a sandcastle",
            "in a city park with autumn leaves"
        ]
    },
    {
        "id_prompt": "A vintage-style poster of a dog",
        "frame_prompt_list": [
            "playing a guitar at a country concert",
            "sitting by a campfire under a starry sky",
            "riding a skateboard through a bustling city",
            "posing in front of a historical landmark",
            "wearing an astronaut suit on the moon"
        ]
    },
    {
        "id_prompt": "A photo of a dog",
        "frame_prompt_list": [
            "dancing to music at a vibrant street festival",
            "chasing a frisbee in a colorful park",
            "wearing sunglasses while relaxing on a beach chair",
            "posing for a photoshoot in a modern art gallery",
            "jumping through a hoop at a circus performance",
            "playing with a group of children at a playground",
            "exploring a retro diner while wearing a bowtie"
        ]
    },
    {
        "id_prompt": "A mystical illustration of a wise wizard with a long, flowing beard",
        "frame_prompt_list": [
            "in a tower filled with ancient tomes and artifacts",
            "casting a spell by the light of a full moon",
            "standing before a magical portal in the forest",
            "summoning a storm over a mountain peak",
            "writing runes in a dusty spellbook",
            "mixing potions in a dimly lit chamber",
            "consulting a crystal ball"
        ]
    },
    {
        "id_prompt": "A pixar style illustration of a dragon",
        "frame_prompt_list": [
            "soaring gracefully through a rainbow sky",
            "nestled among blooming cherry blossoms",
            "playfully splashing in a sparkling lake"
        ]
    },
    {
        "id_prompt": "A whimsical painting of a delicate fairy",
        "frame_prompt_list": [
            "hovering over a moonlit pond",
            "dancing on the petals of a giant flower",
            "spreading fairy dust over a sleeping village",
            "sitting on a mushroom in a magical forest",
            "playing with fireflies at dusk"
        ]
    },
    {
        "id_prompt": "A hyper-realistic digital painting of an elderly gentleman",
        "frame_prompt_list": [
            "wearing a smoking jacket",
            "at a vintage car show",
            "wearing a vineyard owner's attire",
            "on a golf course",
            "at a classical music concert",
            "painting a landscape"
        ]
    },
    {
        "id_prompt": "A vintage-style poster of a ceramic vase with an intricate floral pattern and a glossy, sky-blue glaze",
        "frame_prompt_list": [
            "holding a rare bouquet of flowers",
            "displaying exotic orchids",
            "complementing a corporate decor",
            "containing delicate cherry blossoms",
            "holding a vibrant arrangement of sunflowers",
            "filled with a fresh bouquet of lavender and wild daisies"
        ]
    },
    {
        "id_prompt": "A photo of a happy hedgehog with its cheese",
        "frame_prompt_list": [
            "in an autumn forest",
            "next to a tiny cheese wheel",
            "sitting on a mushroom",
            "under a picnic blanket",
            "amid blooming spring flowers"
        ]
    },
    {
        "id_prompt": "A heartwarming illustration of a friendly troll",
        "frame_prompt_list": [
            "under a stone bridge covered in ivy",
            "guarding a treasure chest in a dark cave",
            "helping travelers across a river",
            "sitting by a campfire in a foggy forest",
            "building a shelter from fallen logs",
            "fishing in a quiet stream at dusk",
            "carving runes into a rock",
            "resting under a large oak tree"
        ]
    },
    {
        "id_prompt": "A quaint illustration of a hobbit",
        "frame_prompt_list": [
            "in a cozy, round door cottage",
            "sitting by a fireplace in a quaint home",
            "working in a garden of vibrant vegetables",
            "enjoying a feast under a starlit sky",
            "reading a book in a sunlit meadow",
            "walking through a peaceful village",
            "celebrating with friends in a rustic tavern",
            "exploring a hidden valley"
        ]
    },
    {
        "id_prompt": "A hyper-realistic digital painting of a young ginger boy with his ball",
        "frame_prompt_list": [
            "leaves scattering in a gentle breeze",
            "standing in a quiet meadow",
            "set against a vibrant sunset",
            "in a busy street of people",
            "by a colorful graffiti wall",
            "amidst a field of blooming wildflowers"
        ]
    },
    {
        "id_prompt": "A cinematic portrait of a man and a woman standing together",
        "frame_prompt_list": [
            "under a sky full of stars",
            "on a bustling city street at night",
            "in a dimly lit jazz club",
            "walking along a sandy beach at sunset",
            "in a cozy coffee shop with large windows",
            "in a vibrant art gallery surrounded by paintings",
            "under an umbrella during a soft rain",
            "on a quiet park bench amidst falling leaves",
            "standing on a rooftop overlooking the city skyline"
        ]
    },
    {
        "id_prompt": "A cinematic portrait of a man, a woman, and a child",
        "frame_prompt_list": [
            "walking in a quiet park",
            "under a starlit sky",
            "by a rustic cabin",
            "on a forest trail",
            "by a peaceful lake",
            "at a vibrant market",
            "in a snowy street",
            "by a carousel",
            "on a picnic blanket"
        ]
    }
]

# åˆå§‹åŒ–æ•°æ®é›†åˆ—è¡¨
data = []

# ä½¿ç”¨ tqdm æ˜¾ç¤ºæ€»è¿›åº¦æ¡
total_iterations = len(model_paths) * len(combinations)
with tqdm(total=total_iterations, desc="Generating Images", unit="image") as pbar:
    # éå†æ¯ä¸ªç»„åˆå’Œæ¨¡å‹
    for combination in combinations:
        id_prompt = combination["id_prompt"]
        frame_prompt_list = combination["frame_prompt_list"]
        
        for model_path in model_paths:
            # å°† frame_prompt_list è½¬æ¢ä¸ºé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
            frame_prompt_str = ",".join(frame_prompt_list)
            
            # è°ƒç”¨ Gradio å®¢æˆ·ç«¯ç”Ÿæˆå›¾ç‰‡
            result = client.predict(
                model_path=model_path,
                id_prompt=id_prompt,
                frame_prompt_list=frame_prompt_str,  # ä½¿ç”¨é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
                precision="fp16",
                seed=32,
                window_length=10,
                alpha_weaken=0.01,
                beta_weaken=0.05,
                alpha_enhance=-0.01,
                beta_enhance=1,
                ipca_drop_out=0,
                use_freeu="false",
                use_same_init_noise="true",
                api_name="/main_gradio"
            )
            
            # ä¿å­˜å›¾ç‰‡åˆ°æœ¬åœ°
            image_name = f"{model_path.split('/')[-1]}_{id_prompt.replace(' ', '_')}.png"
            image_path = os.path.join(generated_images_dir, image_name)
            Image.open(result).save(image_path)
            
            # å°†æ•°æ®æ·»åŠ åˆ°æ•°æ®é›†åˆ—è¡¨
            data.append({
                "model_name": model_path.split('/')[-1],
                "id_prompt": id_prompt,
                "frame_prompt": frame_prompt_str,  # ä¿å­˜é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
                "image": image_path
            })
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.update(1)

# åˆ›å»ºæ•°æ®é›†å¯¹è±¡
dataset = Dataset.from_dict({
    "model_name": [item["model_name"] for item in data],
    "id_prompt": [item["id_prompt"] for item in data],
    "frame_prompt": [item["frame_prompt"] for item in data],
    "image": [item["image"] for item in data]
})

# åˆ›å»ºä¿å­˜æ•°æ®é›†çš„ç›®å½•
os.makedirs(generated_dataset_dir, exist_ok=True)

# ä¿å­˜æ•°æ®é›†åˆ°æœ¬åœ°
dataset.save_to_disk(generated_dataset_dir)

# å°†å›¾åƒåŠ è½½åˆ°æ•°æ®é›†å¹¶æ¨é€åˆ° Hugging Face Hub
from PIL import Image
ds = dataset.map(lambda x: {"Image": Image.open(x["image"])}).remove_columns(["image"])

# æ¨é€åˆ° Hugging Face Hub
ds.push_to_hub("svjack/OnePromptOneStory-Examples")


print(f"æ•°æ®é›†ç”Ÿæˆå¹¶ä¿å­˜å®Œæˆï¼å›¾ç‰‡ä¿å­˜åˆ°ï¼š{generated_images_dir}ï¼Œæ•°æ®é›†ä¿å­˜åˆ°ï¼š{generated_dataset_dir}")
```

```python
!pip install -U gradio_client
!pip install datasets

from datasets import load_dataset
from PIL import Image
import io

# 1. åŠ è½½æ•°æ®é›†
ds = load_dataset("svjack/OnePromptOneStory-Examples")

def bytes_to_image(image_bytes):
    """
    å°†å­—èŠ‚æ•°æ®ï¼ˆbytesï¼‰è½¬æ¢ä¸º PIL.Image å¯¹è±¡ã€‚

    å‚æ•°:
        image_bytes (bytes): å›¾ç‰‡çš„å­—èŠ‚æ•°æ®ã€‚

    è¿”å›:
        PIL.Image: è½¬æ¢åçš„å›¾ç‰‡å¯¹è±¡ã€‚
    """
    # ä½¿ç”¨ io.BytesIO å°†å­—èŠ‚æ•°æ®è½¬æ¢ä¸ºæ–‡ä»¶æµ
    image_stream = io.BytesIO(image_bytes)
    # ä½¿ç”¨ PIL.Image.open æ‰“å¼€å›¾ç‰‡æµ
    image = Image.open(image_stream)
    return image
    

# 2. å®šä¹‰å›¾ç‰‡åˆ†å‰²å‡½æ•°
def split_image(image, sub_image_width=None, sub_image_height=None):
    """
    å°†å›¾ç‰‡åˆ†å‰²æˆå¤šä¸ªå­å›¾ç‰‡ã€‚
    
    å‚æ•°:
        image (PIL.Image): è¾“å…¥çš„å›¾ç‰‡å¯¹è±¡ã€‚
        sub_image_width (int): æ¯ä¸ªå­å›¾ç‰‡çš„å®½åº¦ã€‚å¦‚æœä¸º Noneï¼Œåˆ™ä¸æ°´å¹³åˆ†å‰²ã€‚
        sub_image_height (int): æ¯ä¸ªå­å›¾ç‰‡çš„é«˜åº¦ã€‚å¦‚æœä¸º Noneï¼Œåˆ™ä¸å‚ç›´åˆ†å‰²ã€‚
    
    è¿”å›:
        list: åŒ…å«æ‰€æœ‰å­å›¾ç‰‡çš„åˆ—è¡¨ã€‚
    """
    # è·å–å›¾ç‰‡çš„å®½åº¦å’Œé«˜åº¦
    width, height = image.size
    
    # åˆå§‹åŒ–å­å›¾ç‰‡åˆ—è¡¨
    sub_images = []
    
    # æ°´å¹³åˆ†å‰²
    if sub_image_width is not None:
        # è®¡ç®—å¯ä»¥åˆ†å‰²æˆå¤šå°‘ä¸ªå­å›¾ç‰‡
        num_horizontal = width // sub_image_width
        for i in range(num_horizontal):
            left = i * sub_image_width
            right = (i + 1) * sub_image_width
            # è£å‰ªå›¾ç‰‡
            sub_image = image.crop((left, 0, right, height))
            sub_images.append(sub_image)
    
    # å‚ç›´åˆ†å‰²
    if sub_image_height is not None:
        # è®¡ç®—å¯ä»¥åˆ†å‰²æˆå¤šå°‘ä¸ªå­å›¾ç‰‡
        num_vertical = height // sub_image_height
        for j in range(num_vertical):
            top = j * sub_image_height
            bottom = (j + 1) * sub_image_height
            # è£å‰ªå›¾ç‰‡
            sub_image = image.crop((0, top, width, bottom))
            sub_images.append(sub_image)
    
    # å¦‚æœæ—¢æ²¡æœ‰æ°´å¹³åˆ†å‰²ä¹Ÿæ²¡æœ‰å‚ç›´åˆ†å‰²ï¼Œè¿”å›åŸå›¾
    if not sub_images:
        sub_images.append(image)
    
    return sub_images

# 3. å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥å¤„ç†æ¯ä¸ªæ ·æœ¬
def process_example(example):
    image = example["Image"]
    # è°ƒç”¨åˆ†å‰²å‡½æ•°ï¼Œå‡è®¾æ°´å¹³åˆ†å‰²å®½åº¦ä¸º 1024
    example["sub_images"] = split_image(image, sub_image_width=1024)
    return example

# 4. åº”ç”¨å‡½æ•°åˆ°æ•´ä¸ªæ•°æ®é›†
ds = ds.map(process_example, num_proc = 6)

# 5. æŸ¥çœ‹ç»“æœ
print(ds["train"][0]["sub_images"])

# 6. æ˜¾ç¤ºç¬¬ä¸€å¼ å­å›¾ç‰‡ï¼ˆå¯é€‰ï¼‰
bytes_to_image(ds["train"][0]["sub_images"][0]["bytes"])

bytes_to_image(ds["train"][0]["sub_images"][0]["bytes"]).save("im.png")
```

![im](https://github.com/user-attachments/assets/8df2c81c-ec65-4440-bf48-a9e100ef195f)


- AnimateLCM-SVD I2V model
```bash
git clone https://huggingface.co/spaces/svjack/AnimateLCM-SVD-Genshin-Impact-Demo && cd AnimateLCM-SVD-Genshin-Impact-Demo && pip install -r requirements.txt
python app.py
```

```python
from gradio_client import Client

client = Client("http://127.0.0.1:7860")
result = client.predict(
		"im.png",	# filepath  in 'Upload your image' Image component
		0,	# float (numeric value between 0 and 9223372036854775807) in 'Seed' Slider component
		True,	# bool  in 'Randomize seed' Checkbox component
		20,	# float (numeric value between 1 and 255) in 'Motion bucket id' Slider component
		8,	# float (numeric value between 5 and 30) in 'Frames per second' Slider component
		1.2,	# float (numeric value between 1 and 2) in 'Max guidance scale' Slider component
		1,	# float (numeric value between 1 and 1.5) in 'Min guidance scale' Slider component
		1024,	# float (numeric value between 576 and 2048) in 'Width of input image' Slider component
		1024,	# float (numeric value between 320 and 1152) in 'Height of input image' Slider component
		4,	# float (numeric value between 1 and 20) in 'Num inference steps' Slider component
		api_name="/video"
)
print(result)

from shutil import copy2
from IPython import display
copy2(result[0]["video"], "vid.mp4")
display.Video("vid.mp4", width = 512, height = 512)
```


https://github.com/user-attachments/assets/ca136c1c-3392-4f00-9d24-fa6390e53575


```python
import uuid
from datasets import load_dataset
from PIL import Image
import io
from gradio_client import Client
import os
import argparse

def bytes_to_image(image_bytes):
    image_stream = io.BytesIO(image_bytes)
    image = Image.open(image_stream)
    return image

def split_image(image, sub_image_width=None, sub_image_height=None):
    width, height = image.size
    sub_images = []
    
    if sub_image_width is not None:
        num_horizontal = width // sub_image_width
        for i in range(num_horizontal):
            left = i * sub_image_width
            right = (i + 1) * sub_image_width
            sub_image = image.crop((left, 0, right, height))
            sub_images.append(sub_image)
    
    if sub_image_height is not None:
        num_vertical = height // sub_image_height
        for j in range(num_vertical):
            top = j * sub_image_height
            bottom = (j + 1) * sub_image_height
            sub_image = image.crop((0, top, width, bottom))
            sub_images.append(sub_image)
    
    if not sub_images:
        sub_images.append(image)
    
    return sub_images

def process_example(example):
    image = example["Image"]
    sub_images = split_image(image, sub_image_width=1024)
    
    example["sub_images"] = []
    for sub_image in sub_images:
        image_bytes = io.BytesIO()
        sub_image.save(image_bytes, format="PNG")
        example["sub_images"].append({"bytes": image_bytes.getvalue()})
    
    return example

def generate_video(image_bytes, motion_bucket_id):
    image = bytes_to_image(image_bytes)
    unique_filename = str(uuid.uuid4()) + ".png"
    image.save(unique_filename)
    
    client = Client("http://127.0.0.1:7860")
    result = client.predict(
        unique_filename, 0, True, motion_bucket_id, 8, 1.2, 1, 1024, 1024, 4, api_name="/video"
    )
    
    os.remove(unique_filename)
    
    with open(result[0]["video"], "rb") as video_file:
        video_bytes = video_file.read()
    
    os.remove(result[0]["video"])
    
    return video_bytes

def add_video_to_example(example):
    example["videos"] = []
    for sub_image_dict in example["sub_images"]:
        image_bytes = sub_image_dict["bytes"]
        for motion_bucket_id in [10, 20, 30, 40, 50]:
            video_bytes = generate_video(image_bytes, motion_bucket_id)
            example["videos"].append({"video_bytes": video_bytes, "motion_bucket_id": motion_bucket_id})
    return example

def main(dataset_name, start_index, end_index, output_dir):
    # åŠ è½½æ•°æ®é›†å¹¶é€‰æ‹© "train" å­é›†
    ds = load_dataset(dataset_name)["train"].select(range(start_index, end_index))
    
    # å¤„ç†æ•°æ®é›†
    ds = ds.map(process_example, num_proc=6)
    ds = ds.map(add_video_to_example, num_proc=1)
    
    # ä¿å­˜å¤„ç†åçš„æ•°æ®é›†
    output_path = os.path.join(output_dir, f"dataset_{start_index}_{end_index}")
    ds.save_to_disk(output_path)
    print(f"Dataset saved to {output_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Process a dataset and generate videos.")
    parser.add_argument("--dataset_name", type=str, default="svjack/OnePromptOneStory-Examples", 
                        help="Name of the dataset to process (default: svjack/OnePromptOneStory-Examples)")
    parser.add_argument("--start", type=int, required=True, help="Start index of the dataset")
    parser.add_argument("--end", type=int, required=True, help="End index of the dataset")
    parser.add_argument("--output_dir", type=str, required=True, help="Output directory to save the dataset")
    
    args = parser.parse_args()
    
    main(args.dataset_name, args.start, args.end, args.output_dir)
```

```bash
#!/bin/bash

# æ•°æ®é›†åç§°
DATASET_NAME="svjack/OnePromptOneStory-Examples"

# è¾“å‡ºç›®å½•
OUTPUT_DIR="OnePromptOneStory-Examples"

# æ•°æ®é›†æ€»å¤§å°
TOTAL_ITEMS=98

# æ¯æ¬¡å¤„ç†çš„æ ·æœ¬æ•°é‡
BATCH_SIZE=5

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p "$OUTPUT_DIR"

# å¾ªç¯å¤„ç†æ•°æ®é›†
for ((START=0; START<TOTAL_ITEMS; START+=BATCH_SIZE)); do
    END=$((START + BATCH_SIZE))
    if ((END > TOTAL_ITEMS)); then
        END=$TOTAL_ITEMS
    fi

    echo "Processing items from $START to $((END-1))..."

    # è°ƒç”¨ Python è„šæœ¬
    python Exp.py --dataset_name "$DATASET_NAME" --start "$START" --end "$END" --output_dir "$OUTPUT_DIR"

    echo "Finished processing items from $START to $((END-1))."
done

echo "All items processed and saved to $OUTPUT_DIR."
```

```python
from datasets import load_from_disk, concatenate_datasets
import pathlib 
import os
import pandas as pd
import numpy as np
l = sorted(pd.Series(list(pathlib.Path("OnePromptOneStory-Examples/").rglob("dataset_*"))).map(
    lambda x: x if os.path.isdir(x) else np.nan
).dropna().map(str).values.tolist(), key = lambda x: 
       list(map(int ,x.split("_")[-2:]))
      )
ds_l = list(map(load_from_disk, l))
ds = concatenate_datasets(ds_l)
ds.push_to_hub("svjack/OnePromptOneStory-Examples-Vid-head75")

from datasets import Dataset, load_dataset
from PIL import Image
import io

def bytes_to_image(image_bytes):
    """
    å°†å­—èŠ‚æ•°æ®ï¼ˆbytesï¼‰è½¬æ¢ä¸º PIL.Image å¯¹è±¡ã€‚
    """
    image_stream = io.BytesIO(image_bytes)
    image = Image.open(image_stream)
    return image

# åŠ è½½æ•°æ®é›†
ds = load_dataset("svjack/OnePromptOneStory-Examples-Vid-head75")["train"]

# å®šä¹‰ motion_bucket_ids
motion_bucket_ids = [10, 20, 30, 40, 50]

# åˆ›å»ºä¸€ä¸ªæ–°çš„æ•°æ®é›†åˆ—è¡¨
new_data = []

# éå†åŸå§‹æ•°æ®é›†ä¸­çš„æ¯ä¸€è¡Œ
for example in tqdm(ds):
    sub_images = example["sub_images"]
    videos = example["videos"]
    
    # éå†æ¯ä¸ª sub_image
    for idx, sub_image_dict in enumerate(sub_images):
        sub_image_bytes = sub_image_dict["bytes"]
        sub_image = bytes_to_image(sub_image_bytes)
        
        # è®¡ç®—å¯¹åº”çš„è§†é¢‘ç´¢å¼•
        video_idx = idx * len(motion_bucket_ids)
        
        # éå†æ¯ä¸ª motion_bucket_id å’Œå¯¹åº”çš„è§†é¢‘
        for i, motion_bucket_id in enumerate(motion_bucket_ids):
            video_dict = videos[video_idx + i]
            video_bytes = video_dict["video_bytes"]  # è§†é¢‘çš„äºŒè¿›åˆ¶æ•°æ®
            
            # åˆ›å»ºæ–°çš„æ ·æœ¬ï¼Œä¿ç•™åŸå§‹æ•°æ®çš„æ‰€æœ‰å­—æ®µ
            new_sample = {
                **example,  # ä¿ç•™åŸå§‹æ•°æ®çš„æ‰€æœ‰å­—æ®µ
                "sub_image": sub_image,
                "motion_bucket_id": motion_bucket_id,
                "video": video_bytes  # ç›´æ¥å­˜å‚¨è§†é¢‘çš„äºŒè¿›åˆ¶æ•°æ®
            }

            new_sample = dict(
                filter(lambda t2: t2[0] not in ["sub_images", "videos"], new_sample.items())
            )
            # æ·»åŠ åˆ°æ–°æ•°æ®é›†ä¸­
            new_data.append(new_sample)

print(len(new_data))
# å°†æ–°æ•°æ®è½¬æ¢ä¸º Hugging Face Dataset å¯¹è±¡
new_dataset = Dataset.from_list(new_data)

# æŸ¥çœ‹æ–°æ•°æ®é›†ä¸­çš„ç¬¬ä¸€ä¸ªæ ·æœ¬
#print(new_dataset[0])

new_dataset.push_to_hub("svjack/OnePromptOneStory-Examples-Vid-head75-Exp")
```

```python
from datasets import load_from_disk
from PIL import Image
from IPython import display
import io

def bytes_to_image(image_bytes):
    """
    å°†å­—èŠ‚æ•°æ®ï¼ˆbytesï¼‰è½¬æ¢ä¸º PIL.Image å¯¹è±¡ã€‚

    å‚æ•°:
        image_bytes (bytes): å›¾ç‰‡çš„å­—èŠ‚æ•°æ®ã€‚

    è¿”å›:
        PIL.Image: è½¬æ¢åçš„å›¾ç‰‡å¯¹è±¡ã€‚
    """
    # ä½¿ç”¨ io.BytesIO å°†å­—èŠ‚æ•°æ®è½¬æ¢ä¸ºæ–‡ä»¶æµ
    image_stream = io.BytesIO(image_bytes)
    # ä½¿ç”¨ PIL.Image.open æ‰“å¼€å›¾ç‰‡æµ
    image = Image.open(image_stream)
    return image

# åŠ è½½æ•°æ®é›†
ds = load_from_disk("example_video_dataset")
### ds.push_to_hub("svjack/OnePromptOneStory-Examples-Vid-head2")
### ds.push_to_hub("svjack/OnePromptOneStory-Examples-Vid")

### ds = load_from_disk("anime_video_dataset")
### ds.push_to_hub("svjack/OnePromptOneStory-AnimeStyle-Vid")

# è·å–ç¬¬ä¸€ä¸ªæ ·æœ¬
example = ds[0]

# é€‰æ‹©ç¬¬ idx ä¸ªå­å›¾ç‰‡å’Œå¯¹åº”çš„è§†é¢‘
idx = 2  # å‡è®¾é€‰æ‹©ç¬¬ 2 ä¸ªå­å›¾ç‰‡

# è·å–å­å›¾ç‰‡
sub_image_dict = example["sub_images"][idx]  # è·å–ç¬¬ idx ä¸ªå­å›¾ç‰‡çš„å­—å…¸
sub_image_bytes = sub_image_dict["bytes"]  # è·å–äºŒè¿›åˆ¶æ•°æ®
sub_image = bytes_to_image(sub_image_bytes)  # è¿˜åŸä¸º PIL.Image å¯¹è±¡
sub_image

# è·å–å¯¹åº”çš„è§†é¢‘
# å‡è®¾æ¯ä¸ªå­å›¾ç‰‡ç”Ÿæˆ 5 ä¸ªè§†é¢‘ï¼ˆå¯¹åº”ä¸åŒçš„ motion_bucket_idï¼‰
motion_bucket_ids = [10, 20, 30, 40, 50]  # å‡è®¾ motion_bucket_id çš„å–å€¼
video_idx = idx * len(motion_bucket_ids)  # è®¡ç®—è§†é¢‘çš„èµ·å§‹ç´¢å¼•

# éå†è¯¥å­å›¾ç‰‡å¯¹åº”çš„æ‰€æœ‰è§†é¢‘
for i, motion_bucket_id in enumerate(motion_bucket_ids):
    video_dict = example["videos"][video_idx + i]  # è·å–å¯¹åº”çš„è§†é¢‘å­—å…¸
    video_bytes = video_dict["video_bytes"]  # è·å–äºŒè¿›åˆ¶æ•°æ®

    # å°†äºŒè¿›åˆ¶æ•°æ®ä¿å­˜ä¸º MP4 æ–‡ä»¶
    def bytes_to_video(video_bytes, output_path="output.mp4"):
        """
        å°†äºŒè¿›åˆ¶æ•°æ®è½¬æ¢ä¸ºè§†é¢‘æ–‡ä»¶å¹¶ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„ã€‚

        å‚æ•°:
            video_bytes (bytes): è§†é¢‘çš„äºŒè¿›åˆ¶æ•°æ®ã€‚
            output_path (str): è§†é¢‘æ–‡ä»¶çš„ä¿å­˜è·¯å¾„ã€‚

        è¿”å›:
            str: è§†é¢‘æ–‡ä»¶çš„è·¯å¾„ã€‚
        """
        with open(output_path, "wb") as video_file:
            video_file.write(video_bytes)
        return output_path

    # ä¿å­˜è§†é¢‘å¹¶æ˜¾ç¤º
    video_path = bytes_to_video(video_bytes, f"example_video_{motion_bucket_id}.mp4")  # ä¿å­˜ä¸º MP4 æ–‡ä»¶
    print(f"Displaying video for sub_image {idx} with motion_bucket_id {motion_bucket_id}")
    display.Video(video_path, width=512, height=512)  # åœ¨ Jupyter Notebook ä¸­æ˜¾ç¤ºè§†é¢‘

display.Video("example_video_30.mp4", width=512, height=512)  # åœ¨ Jupyter Notebook ä¸­æ˜¾ç¤ºè§†é¢‘
```

![im1](https://github.com/user-attachments/assets/30ea9f27-7dc9-461e-a55e-1046330fa7dd)



https://github.com/user-attachments/assets/fd41f9b8-3896-4caf-bf62-4b8b449b7271


## Benchmark 

---

### ç‹¬ç«‹è¿è¡Œå‘½ä»¤

1. **`cagliostrolab/animagine-xl-4.0`**
   ```bash
   python -m resource.gen_benchmark \
       --save_dir ./result/benchmark/cagliostrolab_animagine-xl-4.0 \
       --benchmark_path ./resource/consistory+.yaml \
       --device cuda:0 \
       --num_gpus 1 \
       --model_path "cagliostrolab/animagine-xl-4.0"
   ```

2. **`cagliostrolab/animagine-xl-3.1`**
   ```bash
   python -m resource.gen_benchmark \
       --save_dir ./result/benchmark/cagliostrolab_animagine-xl-3.1 \
       --benchmark_path ./resource/consistory+.yaml \
       --device cuda:0 \
       --num_gpus 1 \
       --model_path "cagliostrolab/animagine-xl-3.1"
   ```

3. **`svjack/GenshinImpact_XL_Base`**
   ```bash
   python -m resource.gen_benchmark \
       --save_dir ./result/benchmark/svjack_GenshinImpact_XL_Base \
       --benchmark_path ./resource/consistory+.yaml \
       --device cuda:0 \
       --num_gpus 1 \
       --model_path "svjack/GenshinImpact_XL_Base"
   ```

4. **`stabilityai/stable-diffusion-xl-base-1.0`**
   ```bash
   python -m resource.gen_benchmark \
       --save_dir ./result/benchmark/stabilityai_stable-diffusion-xl-base-1.0 \
       --benchmark_path ./resource/consistory+.yaml \
       --device cuda:0 \
       --num_gpus 1 \
       --model_path "stabilityai/stable-diffusion-xl-base-1.0"
   ```

5. **`RunDiffusion/Juggernaut-X-v10`**
   ```bash
   python -m resource.gen_benchmark \
       --save_dir ./result/benchmark/RunDiffusion_Juggernaut-X-v10 \
       --benchmark_path ./resource/consistory+.yaml \
       --device cuda:0 \
       --num_gpus 1 \
       --model_path "RunDiffusion/Juggernaut-X-v10"
   ```

6. **`playgroundai/playground-v2.5-1024px-aesthetic`**
   ```bash
   python -m resource.gen_benchmark \
       --save_dir ./result/benchmark/playgroundai_playground-v2.5-1024px-aesthetic \
       --benchmark_path ./resource/consistory+.yaml \
       --device cuda:0 \
       --num_gpus 1 \
       --model_path "playgroundai/playground-v2.5-1024px-aesthetic"
   ```

7. **`SG161222/RealVisXL_V4.0`**
   ```bash
   python -m resource.gen_benchmark \
       --save_dir ./result/benchmark/SG161222_RealVisXL_V4.0 \
       --benchmark_path ./resource/consistory+.yaml \
       --device cuda:0 \
       --num_gpus 1 \
       --model_path "SG161222/RealVisXL_V4.0"
   ```

8. **`RunDiffusion/Juggernaut-XI-v11`**
   ```bash
   python -m resource.gen_benchmark \
       --save_dir ./result/benchmark/RunDiffusion_Juggernaut-XI-v11 \
       --benchmark_path ./resource/consistory+.yaml \
       --device cuda:0 \
       --num_gpus 1 \
       --model_path "RunDiffusion/Juggernaut-XI-v11"
   ```

---

### æ•´åˆåˆ° Shell è„šæœ¬

å¦‚æœä½ å¸Œæœ›å°†è¿™äº›å‘½ä»¤æ•´åˆåˆ°ä¸€ä¸ª Shell è„šæœ¬ä¸­ï¼Œå¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ–¹å¼ç¼–å†™ï¼š

```bash
#!/bin/bash

# å®šä¹‰æ¨¡å‹åˆ—è¡¨ï¼ˆsvjack/GenshinImpact_XL_Base æ’åœ¨ Anime æ¨¡å‹ä¹‹åï¼‰
models=(
    "cagliostrolab/animagine-xl-4.0"
    "cagliostrolab/animagine-xl-3.1"
    "svjack/GenshinImpact_XL_Base"
    "stabilityai/stable-diffusion-xl-base-1.0"
    "RunDiffusion/Juggernaut-X-v10"
    "playgroundai/playground-v2.5-1024px-aesthetic"
    "SG161222/RealVisXL_V4.0"
    "RunDiffusion/Juggernaut-XI-v11"
)

# éå†æ¨¡å‹åˆ—è¡¨å¹¶è¿è¡Œä»»åŠ¡
for model_path in "${models[@]}"; do
    # æ ¹æ®æ¨¡å‹åç§°ç”Ÿæˆä¿å­˜è·¯å¾„
    save_dir="./result/benchmark/$(echo $model_path | tr '/' '_')"
    
    echo "Running model: $model_path"
    echo "Saving results to: $save_dir"

    # è¿è¡Œå‘½ä»¤
    python -m resource.gen_benchmark \
        --save_dir "$save_dir" \
        --benchmark_path ./resource/consistory+.yaml \
        --device cuda:0 \
        --num_gpus 1 \
        --model_path "$model_path"

    echo "Finished running model: $model_path"
    echo "----------------------------------------"
done

echo "All tasks completed!"
```

---

### ä½¿ç”¨æ–¹æ³•
1. å°†ä¸Šè¿°è„šæœ¬ä¿å­˜ä¸º `run_models.sh`ã€‚
2. èµ‹äºˆè„šæœ¬æ‰§è¡Œæƒé™ï¼š
   ```bash
   chmod +x run_models.sh
   ```
3. è¿è¡Œè„šæœ¬ï¼š
   ```bash
   ./run_models.sh
   ```

---

### æ€»ç»“
- **ç‹¬ç«‹å‘½ä»¤**ï¼šæ¯ä¸ªæ¨¡å‹çš„è¿è¡Œå‘½ä»¤éƒ½å·²å•ç‹¬åˆ—å‡ºï¼Œæ–¹ä¾¿å•ç‹¬æ‰§è¡Œã€‚
- **Shell è„šæœ¬**ï¼šæ•´åˆæ‰€æœ‰å‘½ä»¤åˆ°ä¸€ä¸ªè„šæœ¬ä¸­ï¼Œæ–¹ä¾¿ä¸€æ¬¡æ€§è¿è¡Œæ‰€æœ‰ä»»åŠ¡ã€‚
- **ä¿å­˜è·¯å¾„**ï¼šæ ¹æ®æ¨¡å‹åç§°è‡ªåŠ¨ç”Ÿæˆä¿å­˜è·¯å¾„ï¼Œç¡®ä¿ç»“æœä¸ä¼šæ··æ·†ã€‚

```python
from datasets import load_dataset, DatasetDict, Dataset
import os
from PIL import Image

# å®šä¹‰æ ¹ç›®å½•
root_dir = "result_cp/benchmark/"

# åˆå§‹åŒ–æ•°æ®é›†å­—å…¸
dataset_dict = {}

# éå† model_name å’Œ animal æ–‡ä»¶å¤¹
model_data = {"model_name": [], "label": [], "image_name": [], "image": [], }  # æ–°å¢ "image_name" åˆ—

for model_name in os.listdir(root_dir):
    model_path = os.path.join(root_dir, model_name)
    if not os.path.isdir(model_path):
        continue

    # åˆå§‹åŒ–å½“å‰ model_name çš„æ•°æ®é›†

    for animal_folder in os.listdir(model_path):
        animal_path = os.path.join(model_path, animal_folder)
        if not os.path.isdir(animal_path):
            continue

        # éå†å›¾åƒæ–‡ä»¶
        for image_file in os.listdir(animal_path):
            image_path = os.path.join(animal_path, image_file)
            if image_file.lower().endswith(('.png', '.jpg', '.jpeg')):
                # æ‰“å¼€å›¾åƒ
                image = Image.open(image_path)
                model_data["image"].append(image)
                model_data["label"].append(animal_folder)
                model_data["model_name"].append(model_name)
                # æ·»åŠ å›¾ç‰‡åç§°ï¼ˆå»æ‰æ‰©å±•åï¼‰
                image_name = os.path.splitext(image_file)[0]  # å»æ‰æ‰©å±•å
                model_data["image_name"].append(image_name)

# å°†å½“å‰ model_name çš„æ•°æ®è½¬æ¢ä¸º Hugging Face Dataset
dataset_dict["train"] = Dataset.from_dict(model_data)

# åˆå¹¶æ‰€æœ‰ model_name çš„æ•°æ®é›†
full_dataset = DatasetDict(dataset_dict)

# æŸ¥çœ‹æ•°æ®é›†
print(full_dataset)

# full_dataset.push_to_hub("svjack/OnePromptOneStory-animagine-xl-4-0-Animal")
```

- CCIP Ranking
```python
###!git clone https://huggingface.co/spaces/svjack/ccip && cd ccip && pip install -r requirements.txt 
###!python app.py

import os
import uuid
from datasets import load_dataset
from PIL import Image
import io
from gradio_client import Client, handle_file

# 1. åŠ è½½æ•°æ®é›†
ds = load_dataset("svjack/OnePromptOneStory-Examples")

def bytes_to_image(image_bytes):
    """
    å°†å­—èŠ‚æ•°æ®ï¼ˆbytesï¼‰è½¬æ¢ä¸º PIL.Image å¯¹è±¡ã€‚

    å‚æ•°:
        image_bytes (bytes): å›¾ç‰‡çš„å­—èŠ‚æ•°æ®ã€‚

    è¿”å›:
        PIL.Image: è½¬æ¢åçš„å›¾ç‰‡å¯¹è±¡ã€‚
    """
    # ä½¿ç”¨ io.BytesIO å°†å­—èŠ‚æ•°æ®è½¬æ¢ä¸ºæ–‡ä»¶æµ
    image_stream = io.BytesIO(image_bytes)
    # ä½¿ç”¨ PIL.Image.open æ‰“å¼€å›¾ç‰‡æµ
    image = Image.open(image_stream)
    return image

# 2. å®šä¹‰å›¾ç‰‡åˆ†å‰²å‡½æ•°
def split_image(image, sub_image_width=None, sub_image_height=None):
    """
    å°†å›¾ç‰‡åˆ†å‰²æˆå¤šä¸ªå­å›¾ç‰‡ã€‚
    
    å‚æ•°:
        image (PIL.Image): è¾“å…¥çš„å›¾ç‰‡å¯¹è±¡ã€‚
        sub_image_width (int): æ¯ä¸ªå­å›¾ç‰‡çš„å®½åº¦ã€‚å¦‚æœä¸º Noneï¼Œåˆ™ä¸æ°´å¹³åˆ†å‰²ã€‚
        sub_image_height (int): æ¯ä¸ªå­å›¾ç‰‡çš„é«˜åº¦ã€‚å¦‚æœä¸º Noneï¼Œåˆ™ä¸å‚ç›´åˆ†å‰²ã€‚
    
    è¿”å›:
        list: åŒ…å«æ‰€æœ‰å­å›¾ç‰‡çš„åˆ—è¡¨ã€‚
    """
    # è·å–å›¾ç‰‡çš„å®½åº¦å’Œé«˜åº¦
    width, height = image.size
    
    # åˆå§‹åŒ–å­å›¾ç‰‡åˆ—è¡¨
    sub_images = []
    
    # æ°´å¹³åˆ†å‰²
    if sub_image_width is not None:
        # è®¡ç®—å¯ä»¥åˆ†å‰²æˆå¤šå°‘ä¸ªå­å›¾ç‰‡
        num_horizontal = width // sub_image_width
        for i in range(num_horizontal):
            left = i * sub_image_width
            right = (i + 1) * sub_image_width
            # è£å‰ªå›¾ç‰‡
            sub_image = image.crop((left, 0, right, height))
            sub_images.append(sub_image)
    
    # å‚ç›´åˆ†å‰²
    if sub_image_height is not None:
        # è®¡ç®—å¯ä»¥åˆ†å‰²æˆå¤šå°‘ä¸ªå­å›¾ç‰‡
        num_vertical = height // sub_image_height
        for j in range(num_vertical):
            top = j * sub_image_height
            bottom = (j + 1) * sub_image_height
            # è£å‰ªå›¾ç‰‡
            sub_image = image.crop((0, top, width, bottom))
            sub_images.append(sub_image)
    
    # å¦‚æœæ—¢æ²¡æœ‰æ°´å¹³åˆ†å‰²ä¹Ÿæ²¡æœ‰å‚ç›´åˆ†å‰²ï¼Œè¿”å›åŸå›¾
    if not sub_images:
        sub_images.append(image)
    
    return sub_images

# 3. å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥å¤„ç†æ¯ä¸ªæ ·æœ¬
def process_example(example):
    image = example["Image"]
    # è°ƒç”¨åˆ†å‰²å‡½æ•°ï¼Œå‡è®¾æ°´å¹³åˆ†å‰²å®½åº¦ä¸º 1024
    example["sub_images"] = split_image(image, sub_image_width=1024)
    return example

# 4. åº”ç”¨å‡½æ•°åˆ°æ•´ä¸ªæ•°æ®é›†
ds = ds.map(process_example, num_proc=6)

# 5. åˆå§‹åŒ– Gradio å®¢æˆ·ç«¯
client = Client("http://127.0.0.1:7860")

# 6. å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥æ¯”è¾ƒå›¾ç‰‡å¹¶ä¿å­˜ç»“æœ
def compare_images(example):
    sub_images = example["sub_images"]
    comparison_results = []
    
    # è·å–ç¬¬ä¸€ä¸ªå­å›¾ç‰‡
    first_image = bytes_to_image(sub_images[0]["bytes"])
    first_image_path = f"{uuid.uuid4()}.png"
    first_image.save(first_image_path)
    
    for i in range(1, len(sub_images)):
        # è·å–å½“å‰å­å›¾ç‰‡
        current_image = bytes_to_image(sub_images[i]["bytes"])
        current_image_path = f"{uuid.uuid4()}.png"
        current_image.save(current_image_path)
        
        # è°ƒç”¨ API æ¯”è¾ƒå›¾ç‰‡
        result = client.predict(
            imagex=handle_file(first_image_path),
            imagey=handle_file(current_image_path),
            model_name="ccip-caformer-24-randaug-pruned",
            api_name="/_compare"
        )
        
        # ä¿å­˜æ¯”è¾ƒç»“æœ
        comparison_results.append(str(result))
        
        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        os.remove(current_image_path)
    
    # åˆ é™¤ç¬¬ä¸€ä¸ªå­å›¾ç‰‡çš„ä¸´æ—¶æ–‡ä»¶
    os.remove(first_image_path)
    
    # å°†æ¯”è¾ƒç»“æœä¿å­˜åˆ°æ•°æ®é›†ä¸­
    example["comparison_results"] = comparison_results
    return example

# 7. åº”ç”¨æ¯”è¾ƒå‡½æ•°åˆ°æ•´ä¸ªæ•°æ®é›†
dss = ds.map(compare_images, num_proc=1)

# 8. æŸ¥çœ‹ç»“æœ
print(dss["train"][0]["comparison_results"])

# 9. ä¿å­˜æ•°æ®é›†ï¼ˆå¯é€‰ï¼‰
#dss.save_to_disk("example_compare")

dss.map(lambda x: {
    "score_list": list(map(lambda y: eval(y)[0] ,x["comparison_results"])),
    "same_list": list(map(lambda y: eval(y)[1] ,x["comparison_results"]))
}).map(
    lambda x: {
        "max_score": max(x["score_list"]),
        "all_same": all(map(lambda y: "not" not in y.lower(), x["same_list"]))
    }
).remove_columns([
    "comparison_results"
]).sort("max_score").push_to_hub("svjack/OnePromptOneStory-Examples-CCIP")
```


## How To Use

```bash
# Clone this repository
$ git clone https://github.com/svjack/1Prompt1Story

# Go into the repository
$ cd 1Prompt1Story

### Install dependencies ###
$ conda create --name 1p1s python=3.10
$ conda activate 1p1s
# choose the right cuda version of your device
$ conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia 
$ conda install conda-forge::transformers 
$ conda install -c conda-forge diffusers
$ pip install opencv-python scipy gradio=4.44.1 sympy==1.13.1
### Install dependencies ENDs ###

# Run sample code
$ python main.py

# Run gradio demo
$ python app.py

# Run Consistory+ benchmark
$ python -m resource.gen_benchmark --save_dir ./result/benchmark --benchmark_path ./resource/consistory+.yaml
```

## License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.


## Citation
If our work assists your research, feel free to give us a star â­ or cite us using:
```
@inproceedings{
liu2025onepromptonestory,
title={One-Prompt-One-Story: Free-Lunch Consistent Text-to-Image Generation Using a Single Prompt},
author={Tao Liu and Kai Wang and Senmao Li and Joost van de Weijer and Fhad Khan and Shiqi Yang and Yaxing Wang and Jian Yang and Mingming Cheng},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=cD1kl2QKv1}
}
```
